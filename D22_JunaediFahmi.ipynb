{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "D22_JunaediFahmi.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juunnn/DTSAI2019/blob/master/D22_JunaediFahmi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIXw_9sfwZpv",
        "colab_type": "text"
      },
      "source": [
        "# Recurrent Neural Network\n",
        "\n",
        "Neural network yang digunakan untuk data yang bersifat sekuensial. Salah satu dari input data atau output data harus bersifat sekuensial. Jika bukan bersifat sekuensial, lebih baik tidak menggunakan RNN. RNN juga bisa digunakan untuk input yang tidak memiliki jumlah yang sama (bervariasi)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtoE50mFw9qa",
        "colab_type": "text"
      },
      "source": [
        "## RNN Step by Step using numpy by Andrew Ng"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM67odaM0WsE",
        "colab_type": "code",
        "outputId": "b9afcfc5-0f15-405a-b08b-3c9c4a59f064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m5p-zPk0pEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp ./drive/My\\ Drive/Day\\ 22/*.py .\n",
        "!python rnn_utils.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icaZ9sM0wVTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from rnn_utils import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj0qvq1mxlqX",
        "colab_type": "text"
      },
      "source": [
        "### Define Simple one cell RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y74gklcXxi0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_cell_forward(xt, a_prev, params):\n",
        "    Wax = params['Wax']\n",
        "    Waa = params['Waa']\n",
        "    Wya = params['Wya']\n",
        "    ba = params['ba']\n",
        "    by = params['by']\n",
        "    \n",
        "    a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev + ba))\n",
        "    yt_pred = softmax(np.dot(Wya, a_next) + by)\n",
        "    \n",
        "    cache = (a_next, a_prev, xt, params)\n",
        "    \n",
        "    return a_next, yt_pred, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rro3hYArzG7z",
        "colab_type": "code",
        "outputId": "6f5757bc-dfda-4446-a6b5-652bafe1894d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "xt = np.random.randn(5,11)\n",
        "Wax = np.random.randn(5,5)\n",
        "\n",
        "a_prev = np.random.randn(5,11)\n",
        "Waa = np.random.randn(5,5)\n",
        "ba = np.random.randn(5,1)\n",
        "\n",
        "Wya = np.random.randn(2,5)\n",
        "by = np.random.randn(2,1)\n",
        "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
        "\n",
        "a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
        "print(xt)\n",
        "print(\"a_next[4] = \", a_next[4])\n",
        "print(\"a_next.shape = \", a_next.shape)\n",
        "print(\"yt_pred[1] =\", yt_pred[1])\n",
        "print(\"yt_pred.shape = \", yt_pred.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.3015387\n",
            "   1.74481176 -0.7612069   0.3190391  -0.24937038  1.46210794]\n",
            " [-2.06014071 -0.3224172  -0.38405435  1.13376944 -1.09989127 -0.17242821\n",
            "  -0.87785842  0.04221375  0.58281521 -1.10061918  1.14472371]\n",
            " [ 0.90159072  0.50249434  0.90085595 -0.68372786 -0.12289023 -0.93576943\n",
            "  -0.26788808  0.53035547 -0.69166075 -0.39675353 -0.6871727 ]\n",
            " [-0.84520564 -0.67124613 -0.0126646  -1.11731035  0.2344157   1.65980218\n",
            "   0.74204416 -0.19183555 -0.88762896 -0.74715829  1.6924546 ]\n",
            " [ 0.05080775 -0.63699565  0.19091548  2.10025514  0.12015895  0.61720311\n",
            "   0.30017032 -0.35224985 -1.1425182  -0.34934272 -0.20889423]]\n",
            "a_next[4] =  [-0.4195092  -0.58551884  0.02124382  0.93519312 -0.51769647  0.99845078\n",
            " -0.99655721  0.96187829 -0.89590176 -0.76704035 -0.99994933]\n",
            "a_next.shape =  (5, 11)\n",
            "yt_pred[1] = [0.79181938 0.59716224 0.79254019 0.17982782 0.77376338 0.96441299\n",
            " 0.0251575  0.95152364 0.59244386 0.01620252 0.27089351]\n",
            "yt_pred.shape =  (2, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMPf-SWb3c0I",
        "colab_type": "text"
      },
      "source": [
        "### Define n-cell-RNN Forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnY5s0Yg3kIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_forward(x, a0, params):\n",
        "    caches =[]\n",
        "    \n",
        "    n_x, m, T_x = x.shape\n",
        "    n_y, n_a = params['Wya'].shape\n",
        "    \n",
        "    a = np.zeros((n_a, m, T_x))\n",
        "    y_pred = np.zeros((n_y, m, T_x))\n",
        "    \n",
        "    a_next = a0\n",
        "    \n",
        "    for t in range(T_x):\n",
        "        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, params)\n",
        "        a[:,:,t] = a_next\n",
        "        y_pred[:,:,t] = yt_pred\n",
        "        \n",
        "        caches.append(cache)\n",
        "        \n",
        "    caches = (caches, x)\n",
        "    \n",
        "    return a, y_pred, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEyAbZRU4gH9",
        "colab_type": "code",
        "outputId": "8dd0164b-e412-43ec-8e86-e03a1dc7179a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "x = np.random.randn(3,10,4)\n",
        "a0 = np.random.randn(5,10)\n",
        "Waa = np.random.randn(5,5)\n",
        "Wax = np.random.randn(5,3)\n",
        "Wya = np.random.randn(2,5)\n",
        "ba = np.random.randn(5,1)\n",
        "by = np.random.randn(2,1)\n",
        "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
        "\n",
        "a, y_pred, caches = rnn_forward(x, a0, parameters)\n",
        "print(\"a[4][1] = \", a[4][1])\n",
        "print(\"a.shape = \", a.shape)\n",
        "print(\"y_pred[1][3] =\", y_pred[1][3])\n",
        "print(\"y_pred.shape = \", y_pred.shape)\n",
        "print(\"caches[1][1][3] =\", caches[1][1][3])\n",
        "print(\"len(caches) = \", len(caches))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a[4][1] =  [-0.64677882  0.99987691  0.95618551  0.29494841]\n",
            "a.shape =  (5, 10, 4)\n",
            "y_pred[1][3] = [0.95827129 0.98733736 0.02832643 0.95810069]\n",
            "y_pred.shape =  (2, 10, 4)\n",
            "caches[1][1][3] = [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
            "len(caches) =  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-BZr1yZ9h76",
        "colab_type": "text"
      },
      "source": [
        "### Define one cell LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW2u0zzO9lf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm_cell_forward(xt, a_prev, c_prev, params):\n",
        "    Wf = params['Wf']\n",
        "    bf = params['bf']\n",
        "    Wi = params['Wi']\n",
        "    bi = params['bi']\n",
        "    Wc = params['Wc']\n",
        "    bc = params['bc']\n",
        "    Wo = params['Wo']\n",
        "    bo = params['bo']\n",
        "    Wy = params['Wy']\n",
        "    by = params['by']\n",
        "    \n",
        "    n_x, m = xt.shape\n",
        "    n_y, n_a = Wy.shape\n",
        "    \n",
        "    concat = np.zeros((n_a + n_x, m))\n",
        "    concat[:n_a, :] = a_prev\n",
        "    concat[n_a:, :] = xt\n",
        "    \n",
        "    ft = sigmoid(np.dot(Wf, concat) + bf)\n",
        "    it = sigmoid(np.dot(Wi, concat) + bi)\n",
        "    cct = np.tanh(np.dot(Wc, concat) + bc)\n",
        "    c_next = ft*c_prev + it*cct\n",
        "    ot = sigmoid(np.dot(Wo, concat) + bo)\n",
        "    a_next = ot*np.tanh(c_next)\n",
        "    \n",
        "    yt_pred = softmax(np.dot(Wy, a_next) + by)\n",
        "    \n",
        "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, params)\n",
        "    \n",
        "    return a_next, c_next, yt_pred, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjfp-Gg3BQan",
        "colab_type": "code",
        "outputId": "e37c2f00-e574-4cce-cccf-30c76da93cb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "xt = np.random.randn(3,10)\n",
        "a_prev = np.random.randn(5,10)\n",
        "c_prev = np.random.randn(5,10)\n",
        "Wf = np.random.randn(5, 5+3)\n",
        "bf = np.random.randn(5,1)\n",
        "Wi = np.random.randn(5, 5+3)\n",
        "bi = np.random.randn(5,1)\n",
        "Wo = np.random.randn(5, 5+3)\n",
        "bo = np.random.randn(5,1)\n",
        "Wc = np.random.randn(5, 5+3)\n",
        "bc = np.random.randn(5,1)\n",
        "Wy = np.random.randn(2,5)\n",
        "by = np.random.randn(2,1)\n",
        "\n",
        "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
        "\n",
        "a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)\n",
        "print(\"a_next[4] = \", a_next[4])\n",
        "print(\"a_next.shape = \", c_next.shape)\n",
        "print(\"c_next[2] = \", c_next[2])\n",
        "print(\"c_next.shape = \", c_next.shape)\n",
        "print(\"yt[1] =\", yt[1])\n",
        "print(\"yt.shape = \", yt.shape)\n",
        "print(\"cache[1][3] =\", cache[1][3])\n",
        "print(\"len(cache) = \", len(cache))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a_next[4] =  [-0.66408471  0.0036921   0.02088357  0.22834167 -0.85575339  0.00138482\n",
            "  0.76566531  0.34631421 -0.00215674  0.43827275]\n",
            "a_next.shape =  (5, 10)\n",
            "c_next[2] =  [ 0.63267805  1.00570849  0.35504474  0.20690913 -1.64566718  0.11832942\n",
            "  0.76449811 -0.0981561  -0.74348425 -0.26810932]\n",
            "c_next.shape =  (5, 10)\n",
            "yt[1] = [0.79913913 0.15986619 0.22412122 0.15606108 0.97057211 0.31146381\n",
            " 0.00943007 0.12666353 0.39380172 0.07828381]\n",
            "yt.shape =  (2, 10)\n",
            "cache[1][3] = [-0.16263996  1.03729328  0.72938082 -0.54101719  0.02752074 -0.30821874\n",
            "  0.07651101 -1.03752894  1.41219977 -0.37647422]\n",
            "len(cache) =  10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc2XLtumBn9Y",
        "colab_type": "text"
      },
      "source": [
        "### Define n-cell-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Tt8kR38BtyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm_forward(x, a0, params):\n",
        "    caches = []\n",
        "    \n",
        "    n_x, m, T_x = x.shape\n",
        "    n_y, n_a = params[\"Wy\"].shape\n",
        "    \n",
        "    a = np.zeros((n_a, m, T_x))\n",
        "    c = np.zeros((n_a, m, T_x))\n",
        "    y = np.zeros((n_y, m, T_x))\n",
        "    \n",
        "    a_next = a0\n",
        "    c_next = np.zeros(a_next.shape)\n",
        "    \n",
        "    for t in range(T_x):\n",
        "        a_next, c_next, yt, cache = lstm_cell_forward(x[:,:,t], a_next, c_next, params)\n",
        "        \n",
        "        a[:,:,t] = a_next\n",
        "        y[:,:,t] = yt\n",
        "        c[:,:,t] = c_next\n",
        "        \n",
        "        caches.append(cache)\n",
        "        \n",
        "    caches = (caches, x)\n",
        "    \n",
        "    return a, y, c, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsPgftyAGAdl",
        "colab_type": "code",
        "outputId": "ecc1cb1b-3e12-4a2e-d4cc-744229be8e62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "x = np.random.randn(3,10,7)\n",
        "a0 = np.random.randn(5,10)\n",
        "Wf = np.random.randn(5, 5+3)\n",
        "bf = np.random.randn(5,1)\n",
        "Wi = np.random.randn(5, 5+3)\n",
        "bi = np.random.randn(5,1)\n",
        "Wo = np.random.randn(5, 5+3)\n",
        "bo = np.random.randn(5,1)\n",
        "Wc = np.random.randn(5, 5+3)\n",
        "bc = np.random.randn(5,1)\n",
        "Wy = np.random.randn(2,5)\n",
        "by = np.random.randn(2,1)\n",
        "\n",
        "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
        "\n",
        "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
        "print(\"a[4][3][6] = \", a[4][3][6])\n",
        "print(\"a.shape = \", a.shape)\n",
        "print(\"y[1][4][3] =\", y[1][4][3])\n",
        "print(\"y.shape = \", y.shape)\n",
        "print(\"caches[1][1[1]] =\", caches[1][1][1])\n",
        "print(\"c[1][2][1]\", c[1][2][1])\n",
        "print(\"len(caches) = \", len(caches))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a[4][3][6] =  0.17211776753291672\n",
            "a.shape =  (5, 10, 7)\n",
            "y[1][4][3] = 0.9508734618501101\n",
            "y.shape =  (2, 10, 7)\n",
            "caches[1][1[1]] = [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n",
            "  0.41005165]\n",
            "c[1][2][1] -0.8555449167181981\n",
            "len(caches) =  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpMCDU79GRyW",
        "colab_type": "text"
      },
      "source": [
        "### Backrprop in RNN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzvcX8wSGhod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_cell_backward(da_next, cache):\n",
        "    a_next, a_prev, xt, params = cache\n",
        "    Wax = params['Wax']\n",
        "    Waa = params['Waa']\n",
        "    Wya = params['Wya']\n",
        "    ba = params['ba']\n",
        "    by = params['by']\n",
        "    \n",
        "    dtanh = (1 - a_next**2) * da_next\n",
        "    \n",
        "    dxt = np.dot(Wax.T, dtanh)\n",
        "    dWax = np.dot(dtanh, xt.T)\n",
        "    \n",
        "    da_prev = np.dot(Waa.T, dtanh)\n",
        "    dWaa = np.dot(dtanh, a_prev.T)\n",
        "    \n",
        "    dba = np.sum(dtanh, axis = 1, keepdims=1)\n",
        "    \n",
        "    gradients = {\n",
        "        'dxt' : dxt,\n",
        "        'da_prev': da_prev,\n",
        "        'dWax': dWax,\n",
        "        'dWaa': dWaa,\n",
        "        'dba': dba\n",
        "    }\n",
        "    \n",
        "    return gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j64siI7xH70X",
        "colab_type": "code",
        "outputId": "57ced902-ad1f-4421-edb2-e55927272f06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "xt = np.random.randn(3,10)\n",
        "a_prev = np.random.randn(5,10)\n",
        "Wax = np.random.randn(5,3)\n",
        "Waa = np.random.randn(5,5)\n",
        "Wya = np.random.randn(2,5)\n",
        "b = np.random.randn(5,1)\n",
        "by = np.random.randn(2,1)\n",
        "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
        "\n",
        "a_next, yt, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
        "\n",
        "da_next = np.random.randn(5,10)\n",
        "gradients = rnn_cell_backward(da_next, cache)\n",
        "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients[\"dxt\"][1][2])\n",
        "print(\"gradients[\\\"dxt\\\"].shape =\", gradients[\"dxt\"].shape)\n",
        "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients[\"da_prev\"][2][3])\n",
        "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients[\"da_prev\"].shape)\n",
        "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
        "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
        "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
        "print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n",
        "print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gradients[\"dxt\"][1][2] = -0.2538766034318368\n",
            "gradients[\"dxt\"].shape = (3, 10)\n",
            "gradients[\"da_prev\"][2][3] = -0.1010548539062439\n",
            "gradients[\"da_prev\"].shape = (5, 10)\n",
            "gradients[\"dWax\"][3][1] = 0.7972223339919303\n",
            "gradients[\"dWax\"].shape = (5, 3)\n",
            "gradients[\"dWaa\"][1][2] = -0.45079547710595086\n",
            "gradients[\"dWaa\"].shape = (5, 5)\n",
            "gradients[\"dba\"][4] = [0.23973971]\n",
            "gradients[\"dba\"].shape = (5, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqYS77yNIGDy",
        "colab_type": "text"
      },
      "source": [
        "### Define BPTT for n-cell-RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzd4ZnV_IJ4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_backward(da, caches):\n",
        "    (caches, x) = caches\n",
        "    (a1, a0, x1, parameters) = caches[0]\n",
        "    \n",
        "    n_a, m, T_x = da.shape\n",
        "    n_x, m = x1.shape\n",
        "    \n",
        "    dx = np.zeros((n_x, m, T_x))\n",
        "    dWax = np.zeros((n_a, n_x))\n",
        "    dWaa = np.zeros((n_a, n_a))\n",
        "    dba = np.zeros((n_a, 1))\n",
        "    da0 = np.zeros((n_a, m))\n",
        "    da_prevt = np.zeros((n_a, m))\n",
        "        \n",
        "    \n",
        "    # Loop through all the time steps\n",
        "    for t in reversed(range(T_x)):\n",
        "        \n",
        "        gradients = rnn_cell_backward(da[:,:,t] + da_prevt, caches[t])\n",
        "        \n",
        "        dxt, da_prevt, dWaxt, dWaat, dbat = gradients[\"dxt\"], gradients[\"da_prev\"], gradients[\"dWax\"], gradients[\"dWaa\"], gradients[\"dba\"]\n",
        "        \n",
        "        dx[:, :, t] = dxt\n",
        "        dWax += dWaxt\n",
        "        dWaa += dWaat\n",
        "        dba += dbat\n",
        "        \n",
        "    \n",
        "    da0 = da_prevt\n",
        "\n",
        "    gradients = {\"dx\": dx, \"da0\": da0, \"dWax\": dWax, \"dWaa\": dWaa,\"dba\": dba}\n",
        "    \n",
        "    return gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHxN18ocIle_",
        "colab_type": "code",
        "outputId": "cc1a4980-a793-4366-9645-4091e651751e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "x = np.random.randn(3,10,4)\n",
        "a0 = np.random.randn(5,10)\n",
        "Wax = np.random.randn(5,3)\n",
        "Waa = np.random.randn(5,5)\n",
        "Wya = np.random.randn(2,5)\n",
        "ba = np.random.randn(5,1)\n",
        "by = np.random.randn(2,1)\n",
        "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
        "a, y, caches = rnn_forward(x, a0, parameters)\n",
        "da = np.random.randn(5, 10, 4)\n",
        "gradients = rnn_backward(da, caches)\n",
        "\n",
        "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients[\"dx\"][1][2])\n",
        "print(\"gradients[\\\"dx\\\"].shape =\", gradients[\"dx\"].shape)\n",
        "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients[\"da0\"][2][3])\n",
        "print(\"gradients[\\\"da0\\\"].shape =\", gradients[\"da0\"].shape)\n",
        "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
        "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
        "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
        "print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n",
        "print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gradients[\"dx\"][1][2] = [ 0.46521365 -0.02874406 -0.01724024 -0.35733727]\n",
            "gradients[\"dx\"].shape = (3, 10, 4)\n",
            "gradients[\"da0\"][2][3] = 0.0064419327912555256\n",
            "gradients[\"da0\"].shape = (5, 10)\n",
            "gradients[\"dWax\"][3][1] = 1.488742629350541\n",
            "gradients[\"dWax\"].shape = (5, 3)\n",
            "gradients[\"dWaa\"][1][2] = -0.1561682539997336\n",
            "gradients[\"dWaa\"].shape = (5, 5)\n",
            "gradients[\"dba\"][4] = [-0.02306383]\n",
            "gradients[\"dba\"].shape = (5, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yC35f85IsMQ",
        "colab_type": "text"
      },
      "source": [
        "### Define BPTT for one-cell-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFJQPtlDIwgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm_cell_backward(da_next, dc_next, cache):\n",
        "    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Retrieve dimensions from xt's and a_next's shape (≈2 lines)\n",
        "    n_x, m = xt.shape\n",
        "    n_a, m = a_next.shape\n",
        "    \n",
        "    # Compute gates related derivatives, you can find their values can be found by looking carefully at equations (7) to (10) (≈4 lines)\n",
        "    dot = da_next * np.tanh(c_next) * ot * (1 - ot)\n",
        "    dcct = (da_next * ot * (1 - np.tanh(c_next) ** 2) + dc_next) * it * (1 - cct ** 2)\n",
        "    dit = (da_next * ot * (1 - np.tanh(c_next) ** 2) + dc_next) * cct * (1 - it) * it\n",
        "    dft = (da_next * ot * (1 - np.tanh(c_next) ** 2) + dc_next) * c_prev * ft * (1 - ft)\n",
        "\n",
        "    # Compute parameters related derivatives. Use equations (11)-(14) (≈8 lines)\n",
        "    \n",
        "    dWf = np.dot(dft, np.hstack([a_prev.T, xt.T]))\n",
        "    dWi = np.dot(dit, np.hstack([a_prev.T, xt.T]))\n",
        "    dWc = np.dot(dcct, np.hstack([a_prev.T, xt.T]))\n",
        "    dWo = np.dot(dot, np.hstack([a_prev.T, xt.T]))\n",
        "    dbf = np.sum(dft, axis=1, keepdims=True)\n",
        "    dbi = np.sum(dit, axis=1, keepdims=True)\n",
        "    dbc = np.sum(dcct, axis=1, keepdims=True)\n",
        "    dbo = np.sum(dot, axis=1, keepdims=True)\n",
        "\n",
        "    # Compute derivatives w.r.t previous hidden state, previous memory state and input. Use equations (15)-(17). (≈3 lines)    \n",
        "    da_prev = np.dot(Wf[:, :n_a].T, dft) + np.dot(Wc[:, :n_a].T, dcct) + np.dot(Wi[:, :n_a].T, dit) + np.dot(Wo[:, :n_a].T, dot)\n",
        "    dc_prev = (da_next * ot * (1 - np.tanh(c_next) ** 2) + dc_next) * ft\n",
        "    dxt = np.dot(Wf[:, n_a:].T, dft) + np.dot(Wc[:, n_a:].T, dcct) + np.dot(Wi[:, n_a:].T, dit) + np.dot(Wo[:, n_a:].T, dot)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Save gradients in dictionary\n",
        "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
        "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
        "\n",
        "    return gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH1kWeZvI43j",
        "colab_type": "code",
        "outputId": "55fb7391-75a1-4a35-f93e-ab47c6e5c6bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "xt = np.random.randn(3,10)\n",
        "a_prev = np.random.randn(5,10)\n",
        "c_prev = np.random.randn(5,10)\n",
        "Wf = np.random.randn(5, 5+3)\n",
        "bf = np.random.randn(5,1)\n",
        "Wi = np.random.randn(5, 5+3)\n",
        "bi = np.random.randn(5,1)\n",
        "Wo = np.random.randn(5, 5+3)\n",
        "bo = np.random.randn(5,1)\n",
        "Wc = np.random.randn(5, 5+3)\n",
        "bc = np.random.randn(5,1)\n",
        "Wy = np.random.randn(2,5)\n",
        "by = np.random.randn(2,1)\n",
        "\n",
        "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
        "\n",
        "a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)\n",
        "\n",
        "da_next = np.random.randn(5,10)\n",
        "dc_next = np.random.randn(5,10)\n",
        "gradients = lstm_cell_backward(da_next, dc_next, cache)\n",
        "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients[\"dxt\"][1][2])\n",
        "print(\"gradients[\\\"dxt\\\"].shape =\", gradients[\"dxt\"].shape)\n",
        "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients[\"da_prev\"][2][3])\n",
        "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients[\"da_prev\"].shape)\n",
        "print(\"gradients[\\\"dc_prev\\\"][2][3] =\", gradients[\"dc_prev\"][2][3])\n",
        "print(\"gradients[\\\"dc_prev\\\"].shape =\", gradients[\"dc_prev\"].shape)\n",
        "print(\"gradients[\\\"dWf\\\"][3][1] =\", gradients[\"dWf\"][3][1])\n",
        "print(\"gradients[\\\"dWf\\\"].shape =\", gradients[\"dWf\"].shape)\n",
        "print(\"gradients[\\\"dWi\\\"][1][2] =\", gradients[\"dWi\"][1][2])\n",
        "print(\"gradients[\\\"dWi\\\"].shape =\", gradients[\"dWi\"].shape)\n",
        "print(\"gradients[\\\"dWc\\\"][3][1] =\", gradients[\"dWc\"][3][1])\n",
        "print(\"gradients[\\\"dWc\\\"].shape =\", gradients[\"dWc\"].shape)\n",
        "print(\"gradients[\\\"dWo\\\"][1][2] =\", gradients[\"dWo\"][1][2])\n",
        "print(\"gradients[\\\"dWo\\\"].shape =\", gradients[\"dWo\"].shape)\n",
        "print(\"gradients[\\\"dbf\\\"][4] =\", gradients[\"dbf\"][4])\n",
        "print(\"gradients[\\\"dbf\\\"].shape =\", gradients[\"dbf\"].shape)\n",
        "print(\"gradients[\\\"dbi\\\"][4] =\", gradients[\"dbi\"][4])\n",
        "print(\"gradients[\\\"dbi\\\"].shape =\", gradients[\"dbi\"].shape)\n",
        "print(\"gradients[\\\"dbc\\\"][4] =\", gradients[\"dbc\"][4])\n",
        "print(\"gradients[\\\"dbc\\\"].shape =\", gradients[\"dbc\"].shape)\n",
        "print(\"gradients[\\\"dbo\\\"][4] =\", gradients[\"dbo\"][4])\n",
        "print(\"gradients[\\\"dbo\\\"].shape =\", gradients[\"dbo\"].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gradients[\"dxt\"][1][2] = 3.230559115109188\n",
            "gradients[\"dxt\"].shape = (3, 10)\n",
            "gradients[\"da_prev\"][2][3] = -0.06396214197109233\n",
            "gradients[\"da_prev\"].shape = (5, 10)\n",
            "gradients[\"dc_prev\"][2][3] = 0.7975220387970015\n",
            "gradients[\"dc_prev\"].shape = (5, 10)\n",
            "gradients[\"dWf\"][3][1] = -0.14795483816449675\n",
            "gradients[\"dWf\"].shape = (5, 8)\n",
            "gradients[\"dWi\"][1][2] = 1.0574980552259903\n",
            "gradients[\"dWi\"].shape = (5, 8)\n",
            "gradients[\"dWc\"][3][1] = 2.304562163687667\n",
            "gradients[\"dWc\"].shape = (5, 8)\n",
            "gradients[\"dWo\"][1][2] = 0.3313115952892109\n",
            "gradients[\"dWo\"].shape = (5, 8)\n",
            "gradients[\"dbf\"][4] = [0.18864637]\n",
            "gradients[\"dbf\"].shape = (5, 1)\n",
            "gradients[\"dbi\"][4] = [-0.40142491]\n",
            "gradients[\"dbi\"].shape = (5, 1)\n",
            "gradients[\"dbc\"][4] = [0.25587763]\n",
            "gradients[\"dbc\"].shape = (5, 1)\n",
            "gradients[\"dbo\"][4] = [0.13893342]\n",
            "gradients[\"dbo\"].shape = (5, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtyJxE1FI_cv",
        "colab_type": "text"
      },
      "source": [
        "### Define BPTT n-cell-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MQba3XXJDPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm_backward(da, caches):\n",
        "    \n",
        "    \"\"\"\n",
        "    Implement the backward pass for the RNN with LSTM-cell (over a whole sequence).\n",
        "\n",
        "    Arguments:\n",
        "    da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x)\n",
        "    dc -- Gradients w.r.t the memory states, numpy-array of shape (n_a, m, T_x)\n",
        "    caches -- cache storing information from the forward pass (lstm_forward)\n",
        "\n",
        "    Returns:\n",
        "    gradients -- python dictionary containing:\n",
        "                        dx -- Gradient of inputs, of shape (n_x, m, T_x)\n",
        "                        da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
        "                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)\n",
        "                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)\n",
        "                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)\n",
        "                        dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve values from the first cache (t=1) of caches.\n",
        "    (caches, x) = caches\n",
        "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Retrieve dimensions from da's and x1's shapes (≈2 lines)\n",
        "    n_a, m, T_x = da.shape\n",
        "    n_x, m = x1.shape\n",
        "    \n",
        "    # initialize the gradients with the right sizes (≈12 lines)\n",
        "    dx = np.zeros((n_x, m, T_x))\n",
        "    da0 = np.zeros((n_a, m))\n",
        "    da_prevt = np.zeros((n_a, m))\n",
        "    dc_prevt = np.zeros((n_a, m))\n",
        "    dWf = np.zeros((n_a, n_a + n_x))\n",
        "    dWi = np.zeros((n_a, n_a + n_x))\n",
        "    dWc = np.zeros((n_a, n_a + n_x))\n",
        "    dWo = np.zeros((n_a, n_a + n_x))\n",
        "    dbf = np.zeros((n_a, 1))\n",
        "    dbi = np.zeros((n_a, 1))\n",
        "    dbc = np.zeros((n_a, 1))\n",
        "    dbo = np.zeros((n_a, 1))\n",
        "    \n",
        "    # loop back over the whole sequence\n",
        "    for t in reversed(range(T_x)):\n",
        "        # Compute all gradients using lstm_cell_backward\n",
        "        gradients = lstm_cell_backward(da[:,:,t] + da_prevt, dc_prevt, caches[t])\n",
        "        # Store or add the gradient to the parameters' previous step's gradient\n",
        "        dx[:,:,t] = gradients[\"dxt\"]\n",
        "        dWf += gradients[\"dWf\"]\n",
        "        dWi += gradients[\"dWi\"]\n",
        "        dWc += gradients[\"dWc\"]\n",
        "        dWo += gradients[\"dWo\"]\n",
        "        dbf += gradients[\"dbf\"]\n",
        "        dbi += gradients[\"dbi\"]\n",
        "        dbc += gradients[\"dbc\"]\n",
        "        dbo += gradients[\"dbo\"]\n",
        "    # Set the first activation's gradient to the backpropagated gradient da_prev.\n",
        "    da0 = gradients[\"da_prev\"]\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Store the gradients in a python dictionary\n",
        "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
        "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
        "    \n",
        "    return gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG1vsBn_JHBW",
        "colab_type": "code",
        "outputId": "5670870f-4c1e-4d3a-a6e8-c35b59d0e052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "x = np.random.randn(3,10,7)\n",
        "a0 = np.random.randn(5,10)\n",
        "Wf = np.random.randn(5, 5+3)\n",
        "bf = np.random.randn(5,1)\n",
        "Wi = np.random.randn(5, 5+3)\n",
        "bi = np.random.randn(5,1)\n",
        "Wo = np.random.randn(5, 5+3)\n",
        "bo = np.random.randn(5,1)\n",
        "Wc = np.random.randn(5, 5+3)\n",
        "bc = np.random.randn(5,1)\n",
        "\n",
        "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
        "\n",
        "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
        "\n",
        "da = np.random.randn(5, 10, 4)\n",
        "gradients = lstm_backward(da, caches)\n",
        "\n",
        "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients[\"dx\"][1][2])\n",
        "print(\"gradients[\\\"dx\\\"].shape =\", gradients[\"dx\"].shape)\n",
        "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients[\"da0\"][2][3])\n",
        "print(\"gradients[\\\"da0\\\"].shape =\", gradients[\"da0\"].shape)\n",
        "print(\"gradients[\\\"dWf\\\"][3][1] =\", gradients[\"dWf\"][3][1])\n",
        "print(\"gradients[\\\"dWf\\\"].shape =\", gradients[\"dWf\"].shape)\n",
        "print(\"gradients[\\\"dWi\\\"][1][2] =\", gradients[\"dWi\"][1][2])\n",
        "print(\"gradients[\\\"dWi\\\"].shape =\", gradients[\"dWi\"].shape)\n",
        "print(\"gradients[\\\"dWc\\\"][3][1] =\", gradients[\"dWc\"][3][1])\n",
        "print(\"gradients[\\\"dWc\\\"].shape =\", gradients[\"dWc\"].shape)\n",
        "print(\"gradients[\\\"dWo\\\"][1][2] =\", gradients[\"dWo\"][1][2])\n",
        "print(\"gradients[\\\"dWo\\\"].shape =\", gradients[\"dWo\"].shape)\n",
        "print(\"gradients[\\\"dbf\\\"][4] =\", gradients[\"dbf\"][4])\n",
        "print(\"gradients[\\\"dbf\\\"].shape =\", gradients[\"dbf\"].shape)\n",
        "print(\"gradients[\\\"dbi\\\"][4] =\", gradients[\"dbi\"][4])\n",
        "print(\"gradients[\\\"dbi\\\"].shape =\", gradients[\"dbi\"].shape)\n",
        "print(\"gradients[\\\"dbc\\\"][4] =\", gradients[\"dbc\"][4])\n",
        "print(\"gradients[\\\"dbc\\\"].shape =\", gradients[\"dbc\"].shape)\n",
        "print(\"gradients[\\\"dbo\\\"][4] =\", gradients[\"dbo\"][4])\n",
        "print(\"gradients[\\\"dbo\\\"].shape =\", gradients[\"dbo\"].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gradients[\"dx\"][1][2] = [-0.00173313  0.08287442 -0.30545663 -0.43281115]\n",
            "gradients[\"dx\"].shape = (3, 10, 4)\n",
            "gradients[\"da0\"][2][3] = -0.0959115019540047\n",
            "gradients[\"da0\"].shape = (5, 10)\n",
            "gradients[\"dWf\"][3][1] = -0.06981985612744013\n",
            "gradients[\"dWf\"].shape = (5, 8)\n",
            "gradients[\"dWi\"][1][2] = 0.1023718202485478\n",
            "gradients[\"dWi\"].shape = (5, 8)\n",
            "gradients[\"dWc\"][3][1] = -0.06249837949274524\n",
            "gradients[\"dWc\"].shape = (5, 8)\n",
            "gradients[\"dWo\"][1][2] = 0.04843891314443014\n",
            "gradients[\"dWo\"].shape = (5, 8)\n",
            "gradients[\"dbf\"][4] = [-0.0565788]\n",
            "gradients[\"dbf\"].shape = (5, 1)\n",
            "gradients[\"dbi\"][4] = [-0.15399065]\n",
            "gradients[\"dbi\"].shape = (5, 1)\n",
            "gradients[\"dbc\"][4] = [-0.29691142]\n",
            "gradients[\"dbc\"].shape = (5, 1)\n",
            "gradients[\"dbo\"][4] = [-0.29798344]\n",
            "gradients[\"dbo\"].shape = (5, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x77Dqar1JODV",
        "colab_type": "text"
      },
      "source": [
        "## Dinosurus Names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FagLc2FJJ439",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp ./drive/My\\ Drive/Day\\ 22/dinos.txt .\n",
        "!python utils.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyxQrQltJvjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from utils import *\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcpWiwv5KQ60",
        "colab_type": "code",
        "outputId": "5481db53-9046-4587-d6c0-1cb9d91e53ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = open('dinos.txt', 'r').read()\n",
        "data = data.lower()\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print(\"Ada {} total karakter, dan {} karakter unik\".format(data_size, vocab_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ada 19909 total karakter, dan 27 karakter unik\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj1AsHewK1ok",
        "colab_type": "code",
        "outputId": "1cfc20eb-dce3-4452-d4dd-fdd1e5e1b9eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
        "ix_to_char"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '\\n',\n",
              " 1: 'a',\n",
              " 2: 'b',\n",
              " 3: 'c',\n",
              " 4: 'd',\n",
              " 5: 'e',\n",
              " 6: 'f',\n",
              " 7: 'g',\n",
              " 8: 'h',\n",
              " 9: 'i',\n",
              " 10: 'j',\n",
              " 11: 'k',\n",
              " 12: 'l',\n",
              " 13: 'm',\n",
              " 14: 'n',\n",
              " 15: 'o',\n",
              " 16: 'p',\n",
              " 17: 'q',\n",
              " 18: 'r',\n",
              " 19: 's',\n",
              " 20: 't',\n",
              " 21: 'u',\n",
              " 22: 'v',\n",
              " 23: 'w',\n",
              " 24: 'x',\n",
              " 25: 'y',\n",
              " 26: 'z'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTli6CN8LBR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clip(grads, max_value):\n",
        "    dWaa, dWax, dWya, db, dby = grads['dWaa'], grads['dWax'], grads['dWya'], grads['db'], grads['dby']\n",
        "    \n",
        "    for grad in [dWax, dWaa, dWya, db, dby] :\n",
        "        np.clip(grad, -max_value, max_value, grad)\n",
        "        \n",
        "    grads = {\n",
        "        'dWaa': dWaa,\n",
        "        'dWax': dWax,\n",
        "        'dWya': dWya,\n",
        "        'db': db,\n",
        "        'dby': dby\n",
        "    }\n",
        "    \n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-b3CAXLL9jV",
        "colab_type": "code",
        "outputId": "4520dbdc-59a8-4f6c-a943-29e0b45d2ddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "np.random.seed(3)\n",
        "dWax = np.random.randn(5,3)*10\n",
        "dWaa = np.random.randn(5,5)*10\n",
        "dWya = np.random.randn(2,5)*10\n",
        "db = np.random.randn(5,1)*10\n",
        "dby = np.random.randn(2,1)*10\n",
        "gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
        "gradients = clip(gradients, 10)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
        "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
        "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
        "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
        "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gradients[\"dWaa\"][1][2] = 10.0\n",
            "gradients[\"dWax\"][3][1] = -10.0\n",
            "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
            "gradients[\"db\"][4] = [10.]\n",
            "gradients[\"dby\"][1] = [8.45833407]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-Mm1uH2MPYB",
        "colab_type": "text"
      },
      "source": [
        "### Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSJWWZ9rMQ8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(params, char_to_ix, seed):\n",
        "    Waa, Wax, Wya, by, b = params['Waa'], params['Wax'], params['Wya'], params['by'], params['b']\n",
        "    vocab_size = by.shape[0]\n",
        "    n_a = Waa.shape[1]\n",
        "    n_x = Wax.shape[1]\n",
        "    \n",
        "    x = np.zeros((n_x, 1))\n",
        "    a_prev = np.zeros((n_a, 1))\n",
        "    \n",
        "    indices = []\n",
        "    \n",
        "    idx = -1\n",
        "    \n",
        "    counter = 0\n",
        "    newline_character = char_to_ix['\\n']\n",
        "    while (idx != newline_character and counter != 50):\n",
        "        \n",
        "        a = np.tanh(np.dot(Wax,x)+np.dot(Waa,a_prev)+b)\n",
        "        z = np.dot(Wya,a)+by\n",
        "        y = softmax(z)\n",
        "        \n",
        "        np.random.seed(counter+seed)\n",
        "        \n",
        "        \n",
        "        idx = np.random.choice(range(0, vocab_size), p=y.ravel())\n",
        "        \n",
        "        indices.append(idx)\n",
        "        \n",
        "        x = np.zeros((vocab_size,1))\n",
        "        x[idx] = 1\n",
        "        a_prev = a\n",
        "        seed += 1\n",
        "        counter +=1\n",
        "        \n",
        "    if (counter == 50):\n",
        "        indices.append(char_to_ix['\\n'])\n",
        "        \n",
        "    return indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMNAC9STPPyS",
        "colab_type": "code",
        "outputId": "2a43c864-69a8-4025-fcb1-fa3d339b1dc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "np.random.seed(2)\n",
        "_, n_a = 20, 100\n",
        "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
        "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
        "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
        "indices = sample(parameters, char_to_ix, 0)\n",
        "print(\"Sampling:\")\n",
        "print(\"list of sampled indices:\", indices)\n",
        "print(\"list of sampled characters:\", [ix_to_char[i] for i in indices])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sampling:\n",
            "list of sampled indices: [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, 7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 11, 17, 24, 12, 13, 24, 0]\n",
            "list of sampled characters: ['l', 'q', 'x', 'n', 'm', 'i', 'j', 'v', 'x', 'f', 'm', 'k', 'l', 'f', 'u', 'o', 'u', 'n', 'c', 'b', 'a', 'u', 'r', 'x', 'g', 'y', 'f', 'y', 'r', 'j', 'p', 'b', 'c', 'h', 'o', 'l', 'k', 'g', 'a', 'l', 'j', 'b', 'g', 'g', 'k', 'q', 'x', 'l', 'm', 'x', '\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVGVGsX4P5i8",
        "colab_type": "text"
      },
      "source": [
        "### Building the language model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGluZzaiP-Jt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
        "    # Forward propagate through time (≈1 line)\n",
        "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
        "    # Backpropagate through time (≈1 line)\n",
        "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
        "    # Clip your gradients between -5 (min) and 5 (max) (≈1 line)\n",
        "    gradients = clip(gradients, 5)\n",
        "    # Update parameters (≈1 line)\n",
        "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "    \n",
        "    return loss, gradients, a[len(X)-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNovPy5CQINp",
        "colab_type": "code",
        "outputId": "ab495c41-cebc-449d-fba9-f3646c2687e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "vocab_size, n_a = 27, 100\n",
        "a_prev = np.random.randn(n_a,1)\n",
        "Wax, Waa, Wya = np.random.randn(n_a, vocab_size),np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
        "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
        "\n",
        "params = {\n",
        "    \"Wax\": Wax,\n",
        "    'Waa': Waa,\n",
        "    'Wya': Wya,\n",
        "    'b' : b,\n",
        "    'by': by\n",
        "}\n",
        "\n",
        "x = [12,3,5,11,22,3]\n",
        "y = [4,14,11,22,25,26]\n",
        "\n",
        "loss, grads, a_last = optimize(x,y,a_prev,params)\n",
        "print(\"Loss =\", loss)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
        "print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\n",
        "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
        "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
        "print(\"gradients[\\\"dby\\\"][1] =\", gradients['dby'][1])\n",
        "print('a_last[4]=',a_last[4])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss = 126.50397572165369\n",
            "gradients[\"dWaa\"][1][2] = 10.0\n",
            "np.argmax(gradients[\"dWax\"]) = 0\n",
            "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
            "gradients[\"db\"][4] = [10.]\n",
            "gradients[\"dby\"][1] = [8.45833407]\n",
            "a_last[4]= [-1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPTgCq9MTgDi",
        "colab_type": "text"
      },
      "source": [
        "### Building model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_mY6H4DTijM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(data, ix_to_char, char_to_ix, num_iter = 35000, n_a = 50, dino_names = 7, vocab_size = 27):\n",
        "    n_x, n_y = vocab_size, vocab_size\n",
        "    params = initialize_parameters(n_a, n_x, n_y)\n",
        "    loss = get_initial_loss(vocab_size, dino_names)\n",
        "\n",
        "    with open('dinos.txt', 'r') as dino:\n",
        "        examples = dino.readlines()\n",
        "\n",
        "    examples = [x.lower().strip() for x in examples]\n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(examples)\n",
        "\n",
        "    a_prev = np.zeros((n_a, 1))\n",
        "    for j in range(num_iter):\n",
        "        idx = j%len(examples)\n",
        "        X = [None] + [char_to_ix[ch] for ch in examples[idx]]\n",
        "        Y = X[1:] + [char_to_ix['\\n']]\n",
        "        curr_loss, grads, a_prev = optimize(X,Y, a_prev, params)\n",
        "        loss = smooth(loss, curr_loss)\n",
        "        if j % 2000 == 0:\n",
        "            print('iteration: {}, Loss {}'.format(j,curr_loss)+'\\n')\n",
        "            seed = 0\n",
        "            for name in range(dino_names):\n",
        "                sampled_indices = sample(params, char_to_ix, seed)\n",
        "                print_sample(sampled_indices, ix_to_char)\n",
        "                seed += 1\n",
        "            print('\\n')\n",
        "\n",
        "    return params\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tnidnaCVve2",
        "colab_type": "code",
        "outputId": "0e6de6c5-4dfb-4c27-b2e7-1b8a896a398c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "params = model(data,ix_to_char, char_to_ix, num_iter=100000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration: 0, Loss 39.548881516330404\n",
            "\n",
            "Nkzxwtdmfqoeyhsqwasjkjvu\n",
            "Kneb\n",
            "Kzxwtdmfqoeyhsqwasjkjvu\n",
            "Neb\n",
            "Zxwtdmfqoeyhsqwasjkjvu\n",
            "Eb\n",
            "Xwtdmfqoeyhsqwasjkjvu\n",
            "\n",
            "\n",
            "iteration: 2000, Loss 25.513792312513992\n",
            "\n",
            "Liusskeomnolxeros\n",
            "Hmdaairus\n",
            "Hytroligoraurus\n",
            "Lecalosapaus\n",
            "Xusicikoraurus\n",
            "Abalpsamantisaurus\n",
            "Tpraneronxeros\n",
            "\n",
            "\n",
            "iteration: 4000, Loss 9.60575347132754\n",
            "\n",
            "Mivrosaurus\n",
            "Inee\n",
            "Ivtroplisaurus\n",
            "Mbaaisaurus\n",
            "Wusichisaurus\n",
            "Cabaselachus\n",
            "Toraperlethosdarenitochusthiamamumamaon\n",
            "\n",
            "\n",
            "iteration: 6000, Loss 37.5588692236861\n",
            "\n",
            "Onwusceomosaurus\n",
            "Lieeaerosaurus\n",
            "Lxussaurus\n",
            "Oma\n",
            "Xusteonosaurus\n",
            "Eeahosaurus\n",
            "Toreonosaurus\n",
            "\n",
            "\n",
            "iteration: 8000, Loss 22.38086721053088\n",
            "\n",
            "Onxusichepriuon\n",
            "Kilabersaurus\n",
            "Lutrodon\n",
            "Omaaerosaurus\n",
            "Xutrcheps\n",
            "Edaksoje\n",
            "Trodiktonus\n",
            "\n",
            "\n",
            "iteration: 10000, Loss 24.54329164944796\n",
            "\n",
            "Onyusaurus\n",
            "Klecalosaurus\n",
            "Lustodon\n",
            "Ola\n",
            "Xusodonia\n",
            "Eeaeosaurus\n",
            "Troceosaurus\n",
            "\n",
            "\n",
            "iteration: 12000, Loss 20.168278742160663\n",
            "\n",
            "Onyxosaurus\n",
            "Kica\n",
            "Lustrepiosaurus\n",
            "Olaagrraiansaurus\n",
            "Yuspangosaurus\n",
            "Eealosaurus\n",
            "Trognesaurus\n",
            "\n",
            "\n",
            "iteration: 14000, Loss 22.28505703376467\n",
            "\n",
            "Meutromodromurus\n",
            "Inda\n",
            "Iutroinatorsaurus\n",
            "Maca\n",
            "Yusteratoptititan\n",
            "Ca\n",
            "Troclosaurus\n",
            "\n",
            "\n",
            "iteration: 16000, Loss 44.012322724972485\n",
            "\n",
            "Meutosaurus\n",
            "Indabdosaurus\n",
            "Itrsaurus\n",
            "Macalosaurus\n",
            "Yuspandon\n",
            "Caahosaurus\n",
            "Trodon\n",
            "\n",
            "\n",
            "iteration: 18000, Loss 18.454324374582644\n",
            "\n",
            "Physshargosaurus\n",
            "Mela\n",
            "Mystrlopholus\n",
            "Pedaeshicarsaurus\n",
            "Yssphonosaurus\n",
            "Eiaesojedis\n",
            "Trodonosaurus\n",
            "\n",
            "\n",
            "iteration: 20000, Loss 26.407774194373587\n",
            "\n",
            "Onyxus\n",
            "Loehalosaurus\n",
            "Mytrrangosaurus\n",
            "Olaabrondanthus\n",
            "Ytrrcharolumus\n",
            "Gaafosaurus\n",
            "Trrchchodylosaurus\n",
            "\n",
            "\n",
            "iteration: 22000, Loss 21.21090776680853\n",
            "\n",
            "Phyusaurus\n",
            "Midacerkia\n",
            "Mustoeomiravenitan\n",
            "Peeaisphadoris\n",
            "Yusoches\n",
            "Habespgeblonomentopseghosaurus\n",
            "Trpangosaurus\n",
            "\n",
            "\n",
            "iteration: 24000, Loss 16.516219350067637\n",
            "\n",
            "Onyxosaurus\n",
            "Licabator\n",
            "Lytroides\n",
            "Ola\n",
            "Yurollesaurus\n",
            "Egadosaurus\n",
            "Trocephieus\n",
            "\n",
            "\n",
            "iteration: 26000, Loss 29.10295986228656\n",
            "\n",
            "Onyusaurus\n",
            "Ligaahusaneosaurus\n",
            "Lustolmanonykusacrimeqtantixan\n",
            "Olaadroncaosaurus\n",
            "Ytrodonosaurus\n",
            "Egagotha\n",
            "Trochenodyisaurus\n",
            "\n",
            "\n",
            "iteration: 28000, Loss 24.897144865748015\n",
            "\n",
            "Plutosaurus\n",
            "Llacahkuracisaurus\n",
            "Mustodonathyptitanchespantiuan\n",
            "Pacaeskacdosaurus\n",
            "Ytrocepheluphiraptor\n",
            "Eiaeptia\n",
            "Trtarasaurus\n",
            "\n",
            "\n",
            "iteration: 30000, Loss 18.236286877378504\n",
            "\n",
            "Oryxisaurus\n",
            "Loca\n",
            "Lustreoenliusaures\n",
            "Pacaisig\n",
            "Xussancophydps\n",
            "Eg\n",
            "Stochisaurus\n",
            "\n",
            "\n",
            "iteration: 32000, Loss 21.977567937085514\n",
            "\n",
            "Nivusaurus\n",
            "Knacalosaurus\n",
            "Lustrasaurus\n",
            "Ndaairus\n",
            "Xuspandon\n",
            "Eg\n",
            "Trohaliomumsaurus\n",
            "\n",
            "\n",
            "iteration: 34000, Loss 31.64926105831491\n",
            "\n",
            "Onyxmigomalex\n",
            "Kica\n",
            "Lutrolomia\n",
            "Ola\n",
            "Xosalonghauglosaurus\n",
            "Ehadon\n",
            "Stlerdor\n",
            "\n",
            "\n",
            "iteration: 36000, Loss 19.33271176049326\n",
            "\n",
            "Meustondeolaurscoeoolirocgrax\n",
            "Inebaeosaurus\n",
            "Jusroia\n",
            "Macalosaurus\n",
            "Yusaurus\n",
            "Eiadryia\n",
            "Trocephiasaurus\n",
            "\n",
            "\n",
            "iteration: 38000, Loss 23.855461024480892\n",
            "\n",
            "Nlusosaurus\n",
            "Lenecersaurus\n",
            "Lustresaurus\n",
            "Nedalosaurus\n",
            "Ystonnis\n",
            "Efalosaurus\n",
            "Srononnoptites\n",
            "\n",
            "\n",
            "iteration: 40000, Loss 18.138822585042544\n",
            "\n",
            "Niusson\n",
            "Licechus\n",
            "Mustrer\n",
            "Ngaacosaurus\n",
            "Yussanloraurosmasaurus\n",
            "Haadosaurus\n",
            "Tronia\n",
            "\n",
            "\n",
            "iteration: 42000, Loss 24.541321775257884\n",
            "\n",
            "Metrus\n",
            "Liecadrus\n",
            "Lystlengcheus\n",
            "Macaertedchylong\n",
            "Yusognatiangosaurus\n",
            "Ga\n",
            "Trolomosaurus\n",
            "\n",
            "\n",
            "iteration: 44000, Loss 14.746633276710906\n",
            "\n",
            "Meutrodon\n",
            "Jledalosaurus\n",
            "Lusspandonax\n",
            "Mecalosaurus\n",
            "Yusihonlonyshosaurus\n",
            "Eeaeosaurus\n",
            "Ushanesaurus\n",
            "\n",
            "\n",
            "iteration: 46000, Loss 35.722739825436925\n",
            "\n",
            "Mitstonia\n",
            "Jieg\n",
            "Lustrionnokussos\n",
            "Medalosaurus\n",
            "Yuspangosaurus\n",
            "Habcosaurus\n",
            "Trolong\n",
            "\n",
            "\n",
            "iteration: 48000, Loss 34.33527817286625\n",
            "\n",
            "Nousslangosaurus\n",
            "Londalosaurus\n",
            "Mustrion\n",
            "Ngcalosaurus\n",
            "Yutrosaurus\n",
            "Habertia\n",
            "Tromilon\n",
            "\n",
            "\n",
            "iteration: 50000, Loss 18.222304428824632\n",
            "\n",
            "Meustriphosaurus\n",
            "Lledamosaurus\n",
            "Lystriphosaurus\n",
            "Macamptia\n",
            "Yuskendonaurops\n",
            "Eeahson\n",
            "Usidon\n",
            "\n",
            "\n",
            "iteration: 52000, Loss 20.348813418790858\n",
            "\n",
            "Metosaurus\n",
            "Jracampton\n",
            "Kutrognasaurus\n",
            "Mecalosaurus\n",
            "Yusmanereitercodon\n",
            "Edantona\n",
            "Trodonosaurus\n",
            "\n",
            "\n",
            "iteration: 54000, Loss 19.45818077499679\n",
            "\n",
            "Mausitan\n",
            "Jiabadosaurus\n",
            "Kutpsaurus\n",
            "Macadpsaurus\n",
            "Yusaurus\n",
            "Edalosaurus\n",
            "Vrollasaurus\n",
            "\n",
            "\n",
            "iteration: 56000, Loss 24.93941455662245\n",
            "\n",
            "Piutosaurus\n",
            "Lica\n",
            "Lustratops\n",
            "Pehalosaurus\n",
            "Ystiloplomus\n",
            "Haaerondantes\n",
            "Usollercesaurus\n",
            "\n",
            "\n",
            "iteration: 58000, Loss 22.05050757180667\n",
            "\n",
            "Meutrognathourus\n",
            "Ingaagphicerorophous\n",
            "Itrrodon\n",
            "Macakronganthpeltrilevanosaurus\n",
            "Yutognathounospasatitia\n",
            "Edantopegtosaurus\n",
            "Trocheriatatitan\n",
            "\n",
            "\n",
            "iteration: 60000, Loss 25.261467211267277\n",
            "\n",
            "Meustrcommidus\n",
            "Knabagosaurus\n",
            "Lussratops\n",
            "Macagosaurus\n",
            "Ystrasaurus\n",
            "Haacosaurus\n",
            "Troideoraurus\n",
            "\n",
            "\n",
            "iteration: 62000, Loss 26.434608552173614\n",
            "\n",
            "Mitstolus\n",
            "Licebassalapsaurus\n",
            "Lutroides\n",
            "Macairus\n",
            "Ystreonosaurus\n",
            "Ga\n",
            "Uskenhus\n",
            "\n",
            "\n",
            "iteration: 64000, Loss 15.297556900451756\n",
            "\n",
            "Meyvus\n",
            "Imbaaescolasaurus\n",
            "Jytrohiansaurus\n",
            "Macaischaansaurus\n",
            "Ytroidesaurus\n",
            "Edalosaurus\n",
            "Vurbicosaurus\n",
            "\n",
            "\n",
            "iteration: 66000, Loss 19.008728149499923\n",
            "\n",
            "Meusaurus\n",
            "Ildaaaptol\n",
            "Juspolongkex\n",
            "Medalosaurus\n",
            "Yslogomimatesaurus\n",
            "Eg\n",
            "Trojlesaurus\n",
            "\n",
            "\n",
            "iteration: 68000, Loss 23.88819381789724\n",
            "\n",
            "Misrus\n",
            "Klacadptheamorsaurus\n",
            "Lussomidmi\n",
            "Ma\n",
            "Ystriphorhylosaurus\n",
            "Ga\n",
            "Wrolopholys\n",
            "\n",
            "\n",
            "iteration: 70000, Loss 14.708322375676802\n",
            "\n",
            "Nousmtasaurus\n",
            "Jiegalosaurus\n",
            "Lusitas\n",
            "Nhabaschachynsaratkeiuchus\n",
            "Yseudraptor\n",
            "Fabdosaurus\n",
            "Vrong\n",
            "\n",
            "\n",
            "iteration: 72000, Loss 18.09562245105548\n",
            "\n",
            "Nousor\n",
            "Klacaeror\n",
            "Lustrarasaurus\n",
            "Nia\n",
            "Ystreposaurus\n",
            "Eea\n",
            "Uskangosaurus\n",
            "\n",
            "\n",
            "iteration: 74000, Loss 22.81386851991324\n",
            "\n",
            "Pltrrodon\n",
            "Jimaberti\n",
            "Justreodon\n",
            "Phabertia\n",
            "Ytrodon\n",
            "Edadosaurus\n",
            "Ussaurus\n",
            "\n",
            "\n",
            "iteration: 76000, Loss 43.0063623291491\n",
            "\n",
            "Plusoranosaurus\n",
            "Lola\n",
            "Mutosaurus\n",
            "Peecestedanossdiaus\n",
            "Ystreoiophus\n",
            "Feaisondelusaurus\n",
            "Wus\n",
            "\n",
            "\n",
            "iteration: 78000, Loss 32.70357020358147\n",
            "\n",
            "Niwusidon\n",
            "Jice\n",
            "Lutroeoimisscriparaptor\n",
            "Ola\n",
            "Ystraphosaurus\n",
            "Ga\n",
            "Ustanhosaurus\n",
            "\n",
            "\n",
            "iteration: 80000, Loss 22.973820891721342\n",
            "\n",
            "Pattrodon\n",
            "Komaacouicansaurus\n",
            "Lustraposaurus\n",
            "Pacaeosaurus\n",
            "Yrosaurus\n",
            "Edalosaurus\n",
            "Ustaodon\n",
            "\n",
            "\n",
            "iteration: 82000, Loss 17.107039238995814\n",
            "\n",
            "Oryxareollomus\n",
            "Jicabcosaurus\n",
            "Lutosaurus\n",
            "Olaaerosasrus\n",
            "Ystonedriandrosaurus\n",
            "Ed\n",
            "Wrokibosaurus\n",
            "\n",
            "\n",
            "iteration: 84000, Loss 26.936223294206034\n",
            "\n",
            "Nivosaurus\n",
            "Jieg\n",
            "Lustriphosaurus\n",
            "Necairopeetthylentimaueseupersijingospondylong\n",
            "Ystriphosaurus\n",
            "Gdalosaurus\n",
            "Wrocerator\n",
            "\n",
            "\n",
            "iteration: 86000, Loss 31.081941600047287\n",
            "\n",
            "Meuscolchus\n",
            "Jiabalosaurus\n",
            "Kustraphia\n",
            "Mabalosaurus\n",
            "Yrosaurus\n",
            "Edalosaurus\n",
            "Wriandromumanucholes\n",
            "\n",
            "\n",
            "iteration: 88000, Loss 14.289291053601309\n",
            "\n",
            "Pitrosaurus\n",
            "Momaacosaurus\n",
            "Mtptodon\n",
            "Pacaeosaurus\n",
            "Ystriphosaurus\n",
            "Ga\n",
            "Wronogon\n",
            "\n",
            "\n",
            "iteration: 90000, Loss 14.965586803598876\n",
            "\n",
            "Nivsitathosaurus\n",
            "Jiceakosaurus\n",
            "Lutosaurus\n",
            "Ndabeson\n",
            "Ystyeresodratos\n",
            "Edaisihachus\n",
            "Wrongdon\n",
            "\n",
            "\n",
            "iteration: 92000, Loss 18.062698445876112\n",
            "\n",
            "Phystrophosaurus\n",
            "Mondcertia\n",
            "Mtrosaurus\n",
            "Paganson\n",
            "Ystrophoratitan\n",
            "Gabeptoi\n",
            "Tronomosaurus\n",
            "\n",
            "\n",
            "iteration: 94000, Loss 18.93437212189177\n",
            "\n",
            "Qlussucoplopterus\n",
            "Mondalosaurus\n",
            "Mtrronodon\n",
            "Qibagrorachyosaurus\n",
            "Ystolong\n",
            "Haadrorcaton\n",
            "Wronodon\n",
            "\n",
            "\n",
            "iteration: 96000, Loss 19.913159905877748\n",
            "\n",
            "Phytrichericrieos\n",
            "Mica\n",
            "Mustraphiocphersaurus\n",
            "Pacaeros\n",
            "Ysticenatosaurus\n",
            "Ga\n",
            "Vrhengosaurus\n",
            "\n",
            "\n",
            "iteration: 98000, Loss 34.41454209605113\n",
            "\n",
            "Rousiodon\n",
            "Mkelalosaurus\n",
            "Mustreodon\n",
            "Rhabespedelus\n",
            "Ystrapior\n",
            "Habesaurus\n",
            "Uslanesaurus\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}